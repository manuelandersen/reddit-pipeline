id,title,selftext,score,num_comments,author,created_utc,url,upvote_ratio,over_18,edited,spoiler,stickied
1ehlebx,Sr. Data Engineer vs excel guy,,1915,78,nobilix,2024-08-01 16:44:38,https://i.redd.it/0q52otpzz2gd1.png,0.91,False,False,False,False
1ehpzgc,Senior vs. Staff Data Engineer,,111,10,durhoward,2024-08-01 19:51:09,https://i.redd.it/4qxlhe5pw3gd1.png,0.9,False,False,False,False
1eh7aux,Which database should I choose for a large database?,"Hello everyone. Currently, I am facing some difficulties in choosing a database. I work at a small company, and we have a project to create a database where molecular biologists can upload data and query other users' data. Due to the nature of molecular biology data, we need a high write throughput (each upload contains about 4 million rows). Therefore, we chose Cassandra because of its fast write speed (tested on our server at 10 million rows / 140s).

However, the current issue is that Cassandra does not have an open-source solution for exporting an API for the frontend to query. If we have to code the backend REST API ourselves, it will be very tiring and time-consuming. I am looking for another database that can do this. I am considering HBase as an alternative solution. Is it really stable? Is there any combo like Directus + Postgres? Please give me your opinions.",49,60,Practical_Slip6791,2024-08-01 03:59:42,https://www.reddit.com/r/dataengineering/comments/1eh7aux/which_database_should_i_choose_for_a_large/,0.97,False,False,False,False
1ehcd6q,Is this book worth reading ? pls give some reviews if anyone of you has read it,,39,8,OpenWeb5282,2024-08-01 09:28:05,https://i.redd.it/yy1fq9i2u0gd1.jpeg,0.92,False,False,False,False
1ehhr08,What is the easiest experience you've had applying for a data engineer job?,How many stages were involved in the process? What sort of questions were you asked?,23,39,Novel-Tree7557,2024-08-01 14:14:34,https://www.reddit.com/r/dataengineering/comments/1ehhr08/what_is_the_easiest_experience_youve_had_applying/,0.87,False,False,False,False
1eh4dyn,What are data engineering tools/systems that have a funny name?,I'll start: Blob,23,37,inner-musician-5457,2024-08-01 01:30:32,https://www.reddit.com/r/dataengineering/comments/1eh4dyn/what_are_data_engineering_toolssystems_that_have/,0.93,False,False,False,False
1eh4hrh,DuckDB in production ,"Is it common for people to use it in production? I‚Äôve seen most people mention it for local processing but I think it would be interesting to set it up in a server but I‚Äôm not sure if this a normal thing people do. 
If you happen to do so, please enlighten me",16,7,Snoo_70708,2024-08-01 01:35:45,https://www.reddit.com/r/dataengineering/comments/1eh4hrh/duckdb_in_production/,0.85,False,False,False,False
1ehc2rt,Is Nifi that Important ?,"I am a junior big data engineer in Poland, in a few days i will be assigned to a project with the following stack (Nifi, Spark, Hive-impala, ozzie) but the nature of the project that is Nifi take the most weight of the project and there is a third party developing spark and i have concerns regarding that i might be tool oriented rather than developer, i need an advice that shall i try to join project that i develop spark myself or reviewing those third party scripts and optimize them would be enough. ",15,5,Ancient_Quiet_790,2024-08-01 09:07:54,https://www.reddit.com/r/dataengineering/comments/1ehc2rt/is_nifi_that_important/,0.84,False,False,False,False
1ehbgfl,Is it a good idea to use Polars with Pyspark?,"Hi, I'm starting to dip my toes into the data engineering world and have been working a little with Spark. I see that the polars library is quite popular for speed and had some questions. Is it a good idea to use Polars on Spark or will it interfere with Spark's parallelism? From what I've been reading Polars seems excellent for parrellelsing stuff on your machine, but I'm unclear how it works with the distributed side of Spark? Any insight on this and related issues would be super helpful.

For context, I'm working on a project where a bunch of parquet files are read into spark, the data then undergoes some cleaning up and transfroming before being merged into a bunch of delta tables.",15,14,Schlooooompy,2024-08-01 08:25:00,https://www.reddit.com/r/dataengineering/comments/1ehbgfl/is_it_a_good_idea_to_use_polars_with_pyspark/,0.86,False,False,False,False
1ehlg5u,"Will Spark speed up my pipeline, Airflow, BigQuery","Hello thanks for taking the time.

I'm a solo developer for an employer who runs a small agency-- basically him and his industry expertise along with me and one other employee. He's brought me on full time because he wants to build out some data services he can offer to his clients with hopes of being able to move away from the traditional agency people / project hand holding strategy into something more scalable-- basically selling curated datasets and expert analysis with some light consulting on the side.

I'm building everything in GCP and pulling from two data sources. Right now the pipeline is running on airflow pulling data from the first API, storing that raw JSON in GCS, as well as performing some transformations on the data to get it to fit our desired schema for BigQuery, saving the transformed JSON, and then running a batch load operation to put the data into BigQuery. Then a request is built off of that data for the second data source (think ""top ten"" or something like that), and the same process is undergone on the second data source.

The process so far is relatively slow, given the API rate limits but also some slowness in processing. I am wary of over-optimizing or over-engineering since I am a single engineer team, and we have other projects ongoing-- however I am curious if someone can answer as to whether it would make sense to perform the data transformations from the raw JSON to the formatted JSON using Spark. Partly this is because I'm interested in learning Spark for its own sake (using the Python API), so there is that. I have of course consulted ChatGPT but it tends to just tell me some version of what I might want to hear i.e. yes you could speed it up with Spark. But what I really want to ask from the human developer perspective is does it make any sense to do it this way or is this over-engineering or inelegant? I'm ok making a bit of an effort to learn the new skill and jerry-rig it to work with my pipeline, like I said I do selfishly just want to apply the tool so that i can put it on my cv, but is this even a useful application of Spark or would I be better of just learning it on some kind of hobby project?

EDIT: Reading the replies and reflecting, sounds like the answer is ""no"", Spark doesn't really make sense here. The main bottleneck is definitely all of the network-bound operations, and looking back the transformations I'm making are minimal, and I've optimized the code pretty much as well as I can using Python (list comprehension :'-D). To be honest, the best way to speed this up will really just be to increase the worker count in Cloud Composer, still a long way for me to go there. I will however consider using an ELT approach next time for this kind of thing, as that was definitely where my mind was going with thinking of introducing Spark. Ultimately I'm still thinking about a way to gain some experience using Spark without myself having to pay to host PB of data! I was hoping I could make it work for this but I'm sure i'll find some opportunity. Thanks for the feedback everyone.",8,8,nem03,2024-08-01 16:46:35,https://www.reddit.com/r/dataengineering/comments/1ehlg5u/will_spark_speed_up_my_pipeline_airflow_bigquery/,0.91,False,False,False,False
1ehkasi,Monthly General Discussion - Aug 2024,"This thread is a place where you can share things that might not warrant their own thread. It is automatically posted each month and you can find previous threads in the collection.

Examples:

* What are you working on this month?
* What was something you accomplished?
* What was something you learned recently?
* What is something frustrating you currently?

As always, sub rules apply. Please be respectful and stay curious.

**Community Links:**

* [Monthly newsletter](https://dataengineeringcommunity.substack.com/)
* [Data Engineering Events](https://dataengineering.wiki/Community/Events)
* [Data Engineering Meetups](https://dataengineering.wiki/Community/Meetups)
* [Get involved in the community](https://dataengineering.wiki/Community/Get+Involved)",5,3,AutoModerator,2024-08-01 16:00:31,https://www.reddit.com/r/dataengineering/comments/1ehkasi/monthly_general_discussion_aug_2024/,0.86,False,False,False,True
1ehcxt6,Pipeline Failure due to bin-log expiry,"I hope everyone using cdc for incremental load has come up with the problem of binlog file expiration. What is the best way to handle these kind of expiration so that the pipeline extracts the data actually from where it has left. 

I guess this problem is more based on the database side rather than on the extractor side. ",4,1,Suspicious_Peanut282,2024-08-01 10:06:00,https://www.reddit.com/r/dataengineering/comments/1ehcxt6/pipeline_failure_due_to_binlog_expiry/,0.84,False,False,False,False
1ehawol,Industry practice for distributed streaming/processing?,"What is considered a good practice for distributed streaming/processing?

Like is it okay to rely on cloud services like MSK, EMR or Kinesis to manage/run jobs? Or is there any other alternatives? Which apache platform should one use?

Any sharing/tips would be very helpfulüôèüèªüôèüèª",3,4,luqmancrit69,2024-08-01 07:46:41,https://www.reddit.com/r/dataengineering/comments/1ehawol/industry_practice_for_distributed/,0.72,False,False,False,False
1eh5fox,Clickhouse on kubernetes,"Hi guys.

I'm currently deploying the Clickhouse on kubernetes.

As you might know, there are several way to deploy Clickhouse on kubernetes such as bitnami helm chart or Altinity clickhouse operator.

  
I just want to know which method is more popular and convenient way to deploy Clickhouse on kubernetes.

If is there any one who has those experiences, please let me know.",3,2,PrimaryConsistent262,2024-08-01 02:21:24,https://www.reddit.com/r/dataengineering/comments/1eh5fox/clickhouse_on_kubernetes/,0.68,False,False,False,False
1eh3rmn,Ideas for this project ,"Hello,

I will be working on a project soon for a marketing agency that generates leads for local businesses and tracks the leads that convert in an Excel sheet. Each business has its own Excel sheet, which they manually update when a customer converts.

The agency has two issues: 

1. Because the leads are updated manually, there are often errors due to minor changes made to the sheet by the companies.

2. The agency wants to scale and will start adding more companies, so this manual process might not be sustainable.

Here's my proposed solution:

1. Create a web application using Flask that will allow the companies to update the database with just a few clicks when clients convert. This will limit the manipulation of the Excel file and reduce manual entry errors.

2. Move from Google Sheets to a more robust database system like MySQL or PostgreSQL with multiple tables to support scalability.

I am looking for more ideas on how to deal with this.

Thank you!",3,4,Key_Perspective6112,2024-08-01 01:00:46,https://www.reddit.com/r/dataengineering/comments/1eh3rmn/ideas_for_this_project/,0.81,False,False,False,False
1ehv52t,What would you do in my situation,"I got a job as a data analyst/data engineer, extracting data from SAP ECC every 4 hours to display insights on powerBI, presently I am using SAP GUI script using VBA to automate EXCEL file extraction every 4 hours and appending the data on MySQL server and connecting it to powerBI. I am using Power Automate script  and using Windows Task Scheduler to schedule it every 4 hours,

  
what would you suggest me to do, what would you do?? I am fresher/newbie so any recommendations are welcome",2,1,Jaapuchkeaa,2024-08-01 23:29:11,https://www.reddit.com/r/dataengineering/comments/1ehv52t/what_would_you_do_in_my_situation/,1.0,False,False,False,False
1ehv1m0,how to append continuous data to PowerBI,"i receive Excel(CSV file) every 4 hours from my manager and need to append the data in MySQL database without dropping an existing database, basically appending it, the data is like start date-1/1/2024 00:00:00 to end data today (every 4 hours) from 9 am to 4 pm, I don't know exact what to do, the columns are same but few transformations are done here and there like concatenating city names 

the pipeline is like

 SAP->excel->mysql->powerBi, I am a newbie, any suggestion would be helpful",2,3,Jaapuchkeaa,2024-08-01 23:24:49,https://www.reddit.com/r/dataengineering/comments/1ehv1m0/how_to_append_continuous_data_to_powerbi/,1.0,False,False,False,False
1ehpcig,Easiest way to automate getting data from Bing/Microsoft Ads data into a Google Sheet?,I need to populate a google sheet with Bing Ads data daily. Is there an easy solution to this?,2,0,rankRascal,2024-08-01 19:24:06,https://www.reddit.com/r/dataengineering/comments/1ehpcig/easiest_way_to_automate_getting_data_from/,0.76,False,False,False,False
1ehnq6m,Is it possible to automate exporting raw data from Google Analytics without using an API call?,"I have a client that has reports on Google Analytics & Google Ads that wants me to set up a datapipe to a s3 bucket to use to create dashboards with Tableau. The reason they want to export the data is because they have several metrics & KPIs that are coming from other sources that they want to integrate the data from Google with. 

I suggested that they create a Google Cloud account to uptilize the API services, but they don‚Äôt want to do that. I‚Äôve been researching deeply into the Google documentation, but can‚Äôt find a solution. 

If anyone has built a similar pipeline let me know please.",2,5,Quick123Fox,2024-08-01 18:18:17,https://i.redd.it/1p8mlmhsg3gd1.jpeg,0.67,False,False,False,False
1ehnlry,Anyone experience with AWS SDLF ,"Hi,

I joined a new company as a data engineer (with \~5 years of experience in Data & Software eng.) 2 weeks ago and was immediately introduced to [AWS SDLF v1.x](https://github.com/awslabs/aws-serverless-data-lake-framework) as the central data lake solution (managed an internal cloud team).  Migration to v2 going to happen until start of Q4 this year. Seems like I will be working with it regularly from now on and write pipelines using Glue / Lambda. 

Currently I am trying to understand the core principles and the way it works under the hood. Compared to previous architectures , this AWS serverless service mess is way more complex and very fragmented. 

Have you worked with SDLF before and what were the main pain points and pitfalls to avoid ? Any tips ? Any positive experiences? Cant find any useful reports about this framework so I am using my chance to ask you here.",2,0,LokeReven,2024-08-01 18:13:24,https://www.reddit.com/r/dataengineering/comments/1ehnlry/anyone_experience_with_aws_sdlf/,0.67,False,False,False,False
1ehn5o1,Is there growth and learning for data engineers in hospital/healthcare companies that use mainly Epic db/dw? ,"It seems like the data system as a whole is mostly maintained by Epic. If anybody is currently working in the data engineer space in a healthcare company, how has your experience been? Please mention the name of the company too if possible. (Can be past or present )",2,1,UnfairDiscount8331,2024-08-01 17:55:33,https://www.reddit.com/r/dataengineering/comments/1ehn5o1/is_there_growth_and_learning_for_data_engineers/,0.76,False,False,False,False
1ehmfq7,Technical vs Subject Matter skills,"In your workplace, what's more valued? Asking both the analytics and DE subreddit.

I work in hospital finance and accounting. We have a dedicated database team, we use mostly Python, SQL, and VBA. I'm easily the most skilled in programming, but I have almost 0 SME compared to my coworkers who have been here much longer. Because of this, I'm unable to contribute to high impact projects, instead I work on automation and troubleshooting our pipelines/scripts (which still requires some degreee of SME to understand the issue). 

My manager knows this and he's good with answering any questions I have, so I'm hopeful about developing my SME in the relativey niche field of hospital finance for years to come.

How will this impact a potential job hunt down the line if I want to stay in hospital finance? We don't use modern cloud or ETL solutions, but rather we use Windows task scheduler, batch scripts, and homegrown ETL with Python, SQL, and VBA. ",2,1,date_uh,2024-08-01 17:26:24,https://www.reddit.com/r/dataengineering/comments/1ehmfq7/technical_vs_subject_matter_skills/,0.67,False,False,False,False
1ehhmzs,How Airbyte 1.0 orchestrates data movement jobs,,4,0,arimbr,2024-08-01 14:09:49,https://airbyte.com/blog/introducing-workloads-how-airbyte-1-0-orchestrates-data-movement-jobs,0.75,False,False,False,False
1ehaxa6,Is anyone else having lots of troubles with the new salesforce connector in data factory?,"Hi,

  
So ever since azure data factory introduced the new salesforce connector im having lots of troubling every day ingesting data from Salesforce into my adls. Sometimes its a connection issue, sometimes data is incorrect, sometimes it cannot retrieve an object.

  
Is anyone else experiencing this issue of has experienced it as well?

  
If yes, what did you do to solve this?",2,0,kbic93,2024-08-01 07:47:48,https://www.reddit.com/r/dataengineering/comments/1ehaxa6/is_anyone_else_having_lots_of_troubles_with_the/,0.76,False,False,False,False
1eh8z70,RDS MySQL -> Snowflake CDC,"Hey fellas, anyone doing this set up that could give me any lead? 

I was looking to have DMS dumping data into s3 then once data loaded in stage snowflake make the appropriate processing to handle duplicates and the logic that comes with the DMS record (the type of update), or is it better do it using a spark job in AWS. Do not have much idea how to handle schema changes, any help very much appreciated ",2,0,josejo9423,2024-08-01 05:38:04,https://www.reddit.com/r/dataengineering/comments/1eh8z70/rds_mysql_snowflake_cdc/,0.67,False,False,False,False
1ehvv3b,Shopify DE role process?,"Thinking about applying for a role at Shopify and am curious if anyone has recently gone through a DE round with them and can provide any insights or tips 
",1,0,bono_my_tires,2024-08-02 00:01:50,https://www.reddit.com/r/dataengineering/comments/1ehvv3b/shopify_de_role_process/,1.0,False,False,False,False
1ehuob0,"glue notebook, scala?",Am I missing something? A new glue notebook defaults to pyspark and I don't see an option to choose scala--not when I create and once it's created its greyed out and locked to pyspark.,1,0,bcsamsquanch,2024-08-01 23:08:18,https://www.reddit.com/r/dataengineering/comments/1ehuob0/glue_notebook_scala/,1.0,False,False,False,False
1ehnidp,Multi Vendor dataset- single product,"How will you prefer to design the system  , given there are multiple dataset with 70 % overlap of the data coverage in facts and dimensions. The datasets are restricted at the hierarchy. But at some point dataset b will fade away, and only data set A will prevail as gold standards. 

For now I am planning to have a mapping key for both systems based on attributes. [ this should take care of de duplication]

Then when time comes I use the flag to delete or archieve the dataset B [ this to me is enterprise driven approach]

Approach 2: I do a union and use a flag [ truncate  load], given that I will always get 48 months of data on monthly basis.",1,1,cida1205,2024-08-01 18:09:30,https://www.reddit.com/r/dataengineering/comments/1ehnidp/multi_vendor_dataset_single_product/,0.57,False,False,False,False
1ehr1qj,Why CSV is still king,,0,4,PiccoloCareful924,2024-08-01 20:34:03,https://konbert.com/blog/why-csv-is-still-king,0.4,False,False,False,False
